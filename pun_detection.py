# -*- coding: utf-8 -*-
"""Pun_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dLU4nxogOtG1W1NyHtp8O3jhuD7NOAaH
"""

!pip install transformers datasets evaluate
!pip install accelerate -U
!pip install transformers[torch]

from huggingface_hub import notebook_login

notebook_login()

from datasets import  load_dataset, DatasetDict, Dataset
from sklearn.model_selection import train_test_split
import pandas as pd

pun_dataset = load_dataset("CreativeLang/pun_detection_semeval2017_task7")

pun_test_dataset = load_dataset("frostymelonade/SemEval2017-task7-pun-detection")

type(pun_dataset)

filtered_homo_dataset = pun_dataset['train'].filter(lambda example: example['type'] == 'homographic')
df = pd.DataFrame(filtered_homo_dataset)
df.head()

traindf, testdf = train_test_split(df, test_size=0.2)

tds = Dataset.from_pandas(traindf)
vds = Dataset.from_pandas(testdf)

filtered_homo_test_dataset = pun_test_dataset['test'].filter(lambda example: example['type'] == 'homographic')
print(filtered_homo_test_dataset)

ds = DatasetDict()
ds['train'] = tds
ds['test'] = vds

print(pun_dataset.keys())

filtered_homo_dataset[0]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_function(examples):
  return tokenizer(examples["text"], truncation=True)

tokenized_data = ds.map(preprocess_function, batched = True)



from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

import evaluate

accuracy = evaluate.load("accuracy")

import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

id2label = {0: "NEGATIVE", 1: "POSITIVE"}
label2id = {"NEGATIVE": 0, "POSITIVE": 1}

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
)

training_args = TrainingArguments(
    output_dir="/content/drive/My Drive/my_model.pt",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    #push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['test'],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

input_sentence = "They threw a party for the inventor of the toaster . And he was toasted ."
input_token = tokenizer(input_sentence, truncation=True)



