{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAQVo_4BVb_h",
        "outputId": "275ab95e-3f68-4bbb-fc2b-a05174f8b4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#using word embeddings and cosine similarity and choose the one with the higest similarity as a pun\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import math\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# IDF\n",
        "def calculate_idf(corpus, word):\n",
        "    # Calculate the number of documents containing words\n",
        "    doc_count = sum(1 for doc in corpus if word in doc)\n",
        "    # Total number of documents\n",
        "    total_docs = len(corpus)\n",
        "    # Calculating IDF\n",
        "    idf = math.log(total_docs / (doc_count + 1))\n",
        "    return idf\n",
        "\n",
        "# NPMI\n",
        "def calculate_NPMI(word1, word2, words, span_size=20):\n",
        "    def calculate_p(x):\n",
        "        return words.count(x) / len(words)\n",
        "\n",
        "    f_x = words.index(word1)\n",
        "    f_y = words.index(word2)\n",
        "    p_x = calculate_p(word1)\n",
        "    p_y = calculate_p(word2)\n",
        "\n",
        "    p_xy = 0\n",
        "    for i in range(max(0, f_x - span_size), min(len(words), f_x + span_size + 1)):\n",
        "        if words[i] == word2:\n",
        "            p_xy += 1\n",
        "    p_xy /= len(words)\n",
        "\n",
        "    npmi = (math.log(p_xy / (p_x * p_y)) - math.log(p_xy)) / (-math.log(p_xy))\n",
        "\n",
        "    return npmi\n",
        "\n",
        "def find_pun_location(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.word_tokenize(s)\n",
        "    t = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    embeddings = {}\n",
        "\n",
        "    for word in t:\n",
        "        embeddings[word] = np.random.rand(300) # embedding dimension = 300\n",
        "\n",
        "    k, b = -1, float('inf')\n",
        "    wk = None\n",
        "\n",
        "    for i in range(len(t)):\n",
        "        ei = embeddings[t[i]]\n",
        "\n",
        "        for j in range(i + 1, len(t)):\n",
        "            ej = embeddings[t[j]]\n",
        "            d = cosine_similarity([ei], [ej])[0][0]\n",
        "\n",
        "            if d < b:\n",
        "                b, k = d, j\n",
        "\n",
        "    if k > -1:\n",
        "        wk = t[k]\n",
        "\n",
        "    else:\n",
        "        # IDF\n",
        "        corpus = [t]\n",
        "        idf_scores = {}\n",
        "\n",
        "        for word in t:\n",
        "            if word not in stop_words:\n",
        "                idf_scores[word] = calculate_idf(corpus, word)\n",
        "\n",
        "        # NPMI\n",
        "        best_pun_word = None\n",
        "        best_score = -1\n",
        "\n",
        "        for i in range(len(t)):\n",
        "            if t[i] not in stop_words:\n",
        "                score = 0\n",
        "                for j in range(len(t)):\n",
        "                    if i != j:\n",
        "                        npmi_score = calculate_NPMI(t[i], t[j], t)\n",
        "                        score += npmi_score\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_pun_word = t[i]\n",
        "\n",
        "        wk = best_pun_word\n",
        "\n",
        "    return wk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "\n",
        "json_file_path = '/content/drive/MyDrive/Colab Notebooks/semeval-task3-homo.json'  # JSON 파일 경로를 지정해야 합니다.\n",
        "\n",
        "sentences = []\n",
        "puns = []\n",
        "predicted =[]\n",
        "\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "for item in data:\n",
        "    sentence = item.get('sentence')\n",
        "    pun = item.get('src')\n",
        "    sentences.append(sentence)\n",
        "    puns.append(pun)\n",
        "\n",
        "for sentence in sentences:\n",
        "    pun_word = find_pun_location(sentence)\n",
        "    predicted.append(pun_word)\n",
        "\n",
        "total_puns = len(puns)\n",
        "total_predicted = len(predicted)\n",
        "\n",
        "if total_puns != total_predicted:\n",
        "    print(\"Error: Different length\")\n",
        "else:\n",
        "\n",
        "    correct_count = sum(1 for i in range(total_puns) if puns[i] == predicted[i])\n",
        "    accuracy = (correct_count / total_puns) * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe878GPiVlNL",
        "outputId": "d548a485-54f7-4681-a875-880eb5980a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Accuracy: 20.42%\n"
          ]
        }
      ]
    }
  ]
}